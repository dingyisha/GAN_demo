{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Flatten, BatchNormalization, Activation, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "message_file = pd.read_csv('AAPL_message_1.csv', header=None)\n",
    "orderbook_file = pd.read_csv('AAPL_orderbook_1.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "def preprocess_data(message_file, orderbook_file):\n",
    "    message_file[0] = message_file[0] / 1000  # Convert time to seconds\n",
    "    message_file[4] = message_file[4] / 10000  # Restore price to its actual value\n",
    "    orderbook_file.iloc[:, ::4] = orderbook_file.iloc[:, ::4] / 10000  # Process order book prices\n",
    "    return message_file, orderbook_file\n",
    "\n",
    "message_file, orderbook_file = preprocess_data(message_file, orderbook_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Message file shape:\", message_file.shape)\n",
    "print(\"Orderbook file shape:\", orderbook_file.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch generator\n",
    "def batch_generator(message_file, orderbook_file, batch_size, history_length):\n",
    "    num_batches = (len(message_file) - history_length) // batch_size\n",
    "    while True:\n",
    "        for i in range(num_batches):\n",
    "            batch_start = i * batch_size\n",
    "            batch_end = batch_start + batch_size + history_length\n",
    "\n",
    "            messages = message_file.iloc[batch_start:batch_end].values\n",
    "            orderbooks = orderbook_file.iloc[batch_start:batch_end].values\n",
    "\n",
    "            history_messages = np.array([messages[j:j+history_length] for j in range(batch_size)])\n",
    "            history_orderbooks = np.array([orderbooks[j:j+history_length] for j in range(batch_size)])\n",
    "\n",
    "            current_messages = messages[history_length:batch_size + history_length]\n",
    "            current_orderbooks = orderbooks[history_length:batch_size + history_length]\n",
    "\n",
    "            yield [history_messages, np.array(current_messages)], [history_orderbooks, np.array(current_orderbooks)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockGAN:\n",
    "    def __init__(self, history_length, noise_length, message_length, orderbook_length, batch_size):\n",
    "        self.history_length = history_length\n",
    "        self.noise_length = noise_length\n",
    "        self.message_length = message_length\n",
    "        self.orderbook_length = orderbook_length\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm_out_length = 128\n",
    "        \n",
    "        self.build_generator()\n",
    "        self.build_critic()\n",
    "        \n",
    "    def build_generator(self):\n",
    "        history_input = Input(shape=(self.history_length, self.message_length))\n",
    "        noise_input = Input(shape=(self.noise_length,))\n",
    "        \n",
    "        lstm_output = LSTM(self.lstm_out_length)(history_input)\n",
    "        gen_input = Concatenate()([lstm_output, noise_input])\n",
    "        \n",
    "        x = Dense(256)(gen_input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dense(self.message_length)(x)  # Ensure the output length matches the current message length\n",
    "        gen_output = Activation('tanh')(x)\n",
    "        \n",
    "        self.generator = Model([history_input, noise_input], gen_output)\n",
    "        \n",
    "    def build_critic(self):\n",
    "        history_input = Input(shape=(self.history_length, self.message_length))\n",
    "        message_input = Input(shape=(self.message_length,))\n",
    "        \n",
    "        lstm_output = LSTM(self.lstm_out_length)(history_input)\n",
    "        critic_input = Concatenate()([lstm_output, message_input])\n",
    "        \n",
    "        x = Dense(256)(critic_input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dense(128)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        critic_output = Dense(1)(x)\n",
    "        \n",
    "        self.critic = Model([history_input, message_input], critic_output)\n",
    "        \n",
    "    def compile(self):\n",
    "        self.generator.compile(optimizer=Adam(0.0001, beta_1=0.5), loss='binary_crossentropy')\n",
    "        self.critic.compile(optimizer=Adam(0.0001, beta_1=0.5), loss=self.wasserstein_loss)\n",
    "        \n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(y_true * y_pred)\n",
    "        \n",
    "    def train(self, message_file, orderbook_file, epochs, batch_size):\n",
    "        batch_gen = batch_generator(message_file, orderbook_file, batch_size, self.history_length)\n",
    "        for epoch in range(epochs):\n",
    "            d_losses = []\n",
    "            g_losses = []\n",
    "            for _ in range((len(message_file) - self.history_length) // batch_size):\n",
    "                [history_messages, current_messages], [history_orderbooks, current_orderbooks] = next(batch_gen)\n",
    "                \n",
    "                # Train the critic\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.noise_length))\n",
    "                generated_messages = self.generator.predict([history_messages, noise])\n",
    "                d_loss_real = self.critic.train_on_batch([history_messages, current_messages], np.ones((batch_size, 1)))\n",
    "                d_loss_fake = self.critic.train_on_batch([history_messages, generated_messages], -np.ones((batch_size, 1)))\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                d_losses.append(d_loss)\n",
    "                \n",
    "                # Train the generator\n",
    "                g_loss = self.generator.train_on_batch([history_messages, noise], np.ones((batch_size, 1)))\n",
    "                g_losses.append(g_loss)\n",
    "                \n",
    "            print(f'{epoch + 1}/{epochs}, d_loss={np.mean(d_losses):.4f}, g_loss={np.mean(g_losses):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and compile the model\n",
    "stock_gan = StockGAN(history_length=20, noise_length=100, message_length=6, orderbook_length=4, batch_size=64)\n",
    "stock_gan.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the number of training epochs and start training\n",
    "stock_gan.train(message_file, orderbook_file, epochs=50, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data using the trained generator\n",
    "def generate_data(generator, history_messages, noise_dim, num_samples):\n",
    "    generated_data = []\n",
    "    for _ in range(num_samples):\n",
    "        noise = np.random.normal(0, 1, (1, noise_dim))\n",
    "        generated_message = generator.predict([history_messages, noise])\n",
    "        generated_data.append(generated_message)\n",
    "    return np.array(generated_data).reshape(num_samples, -1)\n",
    "\n",
    "# Generate sample data using the trained generator\n",
    "num_samples = 1000\n",
    "history_messages_sample = np.random.normal(0, 1, (1, stock_gan.history_length, stock_gan.message_length))\n",
    "generated_messages = generate_data(stock_gan.generator, history_messages_sample, stock_gan.noise_length, num_samples)\n",
    "\n",
    "# Sample real data\n",
    "real_messages_sample = message_file.sample(n=num_samples).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series for comparison\n",
    "def plot_time_series(real_data, generated_data, title):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(real_data, label='Real Data')\n",
    "    plt.plot(generated_data, label='Generated Data', linestyle='--')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot time series of the generated data vs real data\n",
    "for i in range(stock_gan.message_length):\n",
    "    plot_time_series(real_messages_sample[:, i], generated_messages[:, i], f'Time Series of Feature {i+1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
